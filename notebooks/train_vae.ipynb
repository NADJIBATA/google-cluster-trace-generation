{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üöÄ Entra√Ænement du VAE\n",
    "\n",
    "Notebook pour entra√Æner le VAE sur les s√©quences de workload.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üì¶ Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances si n√©cessaire\n",
    "# !pip install torch matplotlib tqdm numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ ENTRA√éNEMENT DU VAE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mount_drive"
   },
   "source": [
    "## üíæ Montage de Google Drive (optionnel)\n",
    "\n",
    "Si vos donn√©es sont sur Google Drive, d√©commentez et ex√©cutez cette cellule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drive_mount"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_definition"
   },
   "source": [
    "## üèóÔ∏è D√©finition du mod√®le VAE\n",
    "\n",
    "Si vous n'avez pas le code du mod√®le, d√©finissez-le ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vae_model"
   },
   "outputs": [],
   "source": [
    "# Importer les classes VAE LSTM\n",
    "from src.models.vae_lstm import VAELSTM as VAE\n",
    "from src.training.losses import vae_loss, VAELossTracker\n",
    "\n",
    "print(\"‚úì Mod√®les import√©s depuis src/models/vae_lstm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configuration"
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Donn√©es\n",
    "    'data_dir': 'data/processed/sequences',\n",
    "    \n",
    "    # Architecture VAE\n",
    "    'input_dim': 100,           # Longueur s√©quence √ó n_features\n",
    "    'latent_dim': 32,           # Dimension espace latent\n",
    "    'hidden_dims': [256, 128],  # Couches cach√©es encoder/decoder\n",
    "    'activation': 'relu',\n",
    "    'dropout': 0.1,\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    'n_epochs': 100,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 1e-3,\n",
    "    'beta': 1.0,                # Poids KL divergence (Œ≤-VAE)\n",
    "    \n",
    "    # Sauvegarde\n",
    "    'checkpoint_dir': 'checkpoints',\n",
    "    'save_every': 10,           # Sauvegarder tous les N epochs\n",
    "    \n",
    "    # Device\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "print(f\"‚öôÔ∏è  Configuration:\")\n",
    "print(f\"   Device: {CONFIG['device']}\")\n",
    "print(f\"   Latent dim: {CONFIG['latent_dim']}\")\n",
    "print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"   Epochs: {CONFIG['n_epochs']}\")\n",
    "print(f\"   Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"   Beta (KL weight): {CONFIG['beta']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_loading"
   },
   "source": [
    "## üìÇ Chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìÇ Chargement des donn√©es\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "data_dir = Path(CONFIG['data_dir'])\n",
    "\n",
    "# Charger s√©quences\n",
    "train_data = np.load(data_dir / 'train.npy')\n",
    "val_data = np.load(data_dir / 'val.npy')\n",
    "\n",
    "print(f\"‚úì Train raw: {train_data.shape}\")\n",
    "print(f\"‚úì Val raw:   {val_data.shape}\")\n",
    "\n",
    "# Si le mod√®le import√© est un LSTM-based VAE, conserver la forme s√©quentielle\n",
    "is_lstm_vae = False\n",
    "try:\n",
    "    is_lstm_vae = hasattr(VAE, '__name__') and ('LSTM' in VAE.__name__.upper() or 'LSTMVAE' in VAE.__name__.upper())\n",
    "except NameError:\n",
    "    is_lstm_vae = False\n",
    "\n",
    "if is_lstm_vae:\n",
    "    # Attendu: (n_samples, seq_len, n_features)\n",
    "    # Si les donn√©es sont d√©j√† en 3D, gardez-les. Si elles sont en 2D (flatten), essayez d'inf√©rer seq_len et input_size.\n",
    "    if train_data.ndim == 3:\n",
    "        seq_train = train_data\n",
    "    elif train_data.ndim == 2:\n",
    "        # Essayer d'utiliser CONFIG pour restaurer la forme\n",
    "        seq_len = CONFIG.get('sequence_length')\n",
    "        input_size = CONFIG.get('input_size')\n",
    "        if seq_len is None or input_size is None:\n",
    "            # tenter d'inf√©rer: supposer input_size=1\n",
    "            input_size = CONFIG.get('input_size', 1)\n",
    "            if CONFIG.get('input_dim'):\n",
    "                seq_len = CONFIG['input_dim'] // input_size\n",
    "            else:\n",
    "                # fallback: prendre sqrt approximation\n",
    "                seq_len = train_data.shape[1] // input_size\n",
    "        train_data = train_data.reshape(len(train_data), seq_len, input_size)\n",
    "        val_data = val_data.reshape(len(val_data), seq_len, input_size)\n",
    "\n",
    "    print(f\"‚úì Donn√©es format√©es pour LSTM-VAE: {train_data.shape}\")\n",
    "\n",
    "    # Cr√©er DataLoaders (s√©quences 3D)\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(train_data))\n",
    "    val_dataset = TensorDataset(torch.FloatTensor(val_data))\n",
    "else:\n",
    "    # Flatten pour VAE dense (batch, seq_len*features)\n",
    "    train_flat = train_data.reshape(len(train_data), -1)\n",
    "    val_flat = val_data.reshape(len(val_data), -1)\n",
    "\n",
    "    print(f\"\\nApr√®s flatten:\")\n",
    "    print(f\"  Train: {train_flat.shape}\")\n",
    "    print(f\"  Val:   {val_flat.shape}\")\n",
    "\n",
    "    # Cr√©er DataLoaders\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(train_flat))\n",
    "    val_dataset = TensorDataset(torch.FloatTensor(val_flat))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì DataLoaders cr√©√©s:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches:   {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_creation"
   },
   "source": [
    "## üèóÔ∏è Cr√©ation du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_model"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üèóÔ∏è  Cr√©ation du mod√®le VAE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import inspect\n",
    "\n",
    "def instantiate_vae_class(VAE_cls):\n",
    "    sig = inspect.signature(VAE_cls.__init__)\n",
    "    param_names = set(sig.parameters.keys())\n",
    "    param_names.discard('self')\n",
    "\n",
    "    # Old dense VAE signature\n",
    "    if 'input_dim' in param_names:\n",
    "        return VAE_cls(\n",
    "            input_dim=CONFIG['input_dim'],\n",
    "            latent_dim=CONFIG['latent_dim'],\n",
    "            hidden_dims=CONFIG.get('hidden_dims', [256, 128]),\n",
    "            activation=CONFIG.get('activation', 'relu'),\n",
    "            dropout=CONFIG.get('dropout', 0.1)\n",
    "        )\n",
    "\n",
    "    # LSTM VAE signature\n",
    "    if 'input_size' in param_names or 'sequence_length' in param_names:\n",
    "        try:\n",
    "            seq_len = train_data.shape[1]\n",
    "            input_size = train_data.shape[2] if train_data.ndim == 3 else 1\n",
    "        except NameError:\n",
    "            seq_len = CONFIG.get('sequence_length', 288)\n",
    "            input_size = CONFIG.get('input_size', 1)\n",
    "\n",
    "        hidden_size = CONFIG.get('hidden_size', CONFIG.get('hidden_dims', [128])[0])\n",
    "        latent_dim = CONFIG['latent_dim']\n",
    "        num_layers = CONFIG.get('num_layers', 2)\n",
    "        dropout = CONFIG.get('dropout', 0.1)\n",
    "        bidirectional = CONFIG.get('bidirectional', False)\n",
    "\n",
    "        return VAE_cls(\n",
    "            input_size=input_size,\n",
    "            sequence_length=seq_len,\n",
    "            hidden_size=hidden_size,\n",
    "            latent_dim=latent_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "    # Fallback: try simple positional init\n",
    "    try:\n",
    "        return VAE_cls(CONFIG['input_dim'], CONFIG['latent_dim'])\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Impossible d'instancier le mod√®le VAE: {e}\")\n",
    "\n",
    "# Instantiate\n",
    "model = instantiate_vae_class(VAE)\n",
    "model = model.to(CONFIG['device'])\n",
    "\n",
    "print(f\"‚úì Mod√®le cr√©√©: {type(model).__name__}\")\n",
    "print(f\"  Device: {CONFIG['device']}\")\n",
    "# Param√®tres si disponible\n",
    "try:\n",
    "    print(f\"  Param√®tres: {model.count_parameters():,}\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Optimiseur\n",
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "print(f\"\\n‚úì Optimiseur: Adam (lr={CONFIG['learning_rate']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## üî• Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üî• Entra√Ænement\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Tracking\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_recon': [],\n",
    "    'train_kl': [],\n",
    "    'val_loss': [],\n",
    "    'val_recon': [],\n",
    "    'val_kl': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Cr√©er dossier checkpoints\n",
    "Path(CONFIG['checkpoint_dir']).mkdir(exist_ok=True)\n",
    "\n",
    "for epoch in tqdm(range(CONFIG['n_epochs']), desc=\"Epochs\"):\n",
    "    \n",
    "    # ========== TRAINING ==========\n",
    "    model.train()\n",
    "    train_tracker = VAELossTracker()\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        x = batch[0].to(CONFIG['device'])\n",
    "        \n",
    "        # Forward\n",
    "        x_recon, mu, log_var = model(x)\n",
    "        \n",
    "        # Loss\n",
    "        loss, recon_loss, kl_div = vae_loss(\n",
    "            x_recon, x, mu, log_var, \n",
    "            beta=CONFIG['beta']\n",
    "        )\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track\n",
    "        train_tracker.update(\n",
    "            loss.item(), \n",
    "            recon_loss.item(), \n",
    "            kl_div.item()\n",
    "        )\n",
    "    \n",
    "    train_losses = train_tracker.get_average()\n",
    "    \n",
    "    # ========== VALIDATION ==========\n",
    "    model.eval()\n",
    "    val_tracker = VAELossTracker()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x = batch[0].to(CONFIG['device'])\n",
    "            \n",
    "            x_recon, mu, log_var = model(x)\n",
    "            loss, recon_loss, kl_div = vae_loss(\n",
    "                x_recon, x, mu, log_var,\n",
    "                beta=CONFIG['beta']\n",
    "            )\n",
    "            \n",
    "            val_tracker.update(\n",
    "                loss.item(),\n",
    "                recon_loss.item(),\n",
    "                kl_div.item()\n",
    "            )\n",
    "    \n",
    "    val_losses = val_tracker.get_average()\n",
    "    \n",
    "    # ========== LOGGING ==========\n",
    "    history['train_loss'].append(train_losses['total'])\n",
    "    history['train_recon'].append(train_losses['recon'])\n",
    "    history['train_kl'].append(train_losses['kl'])\n",
    "    history['val_loss'].append(val_losses['total'])\n",
    "    history['val_recon'].append(val_losses['recon'])\n",
    "    history['val_kl'].append(val_losses['kl'])\n",
    "    \n",
    "    # Print\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"\\nEpoch {epoch+1:3d}/{CONFIG['n_epochs']} | \"\n",
    "              f\"Train Loss: {train_losses['total']:.4f} \"\n",
    "              f\"(R: {train_losses['recon']:.4f}, KL: {train_losses['kl']:.4f}) | \"\n",
    "              f\"Val Loss: {val_losses['total']:.4f}\")\n",
    "    \n",
    "    # ========== CHECKPOINTING ==========\n",
    "    \n",
    "    # Sauvegarder meilleur mod√®le\n",
    "    if val_losses['total'] < best_val_loss:\n",
    "        best_val_loss = val_losses['total']\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': best_val_loss,\n",
    "            'config': CONFIG\n",
    "        }, Path(CONFIG['checkpoint_dir']) / 'vae_best.pth')\n",
    "    \n",
    "    # Sauvegarder p√©riodiquement\n",
    "    if (epoch + 1) % CONFIG['save_every'] == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'history': history,\n",
    "            'config': CONFIG\n",
    "        }, Path(CONFIG['checkpoint_dir']) / f'vae_epoch_{epoch+1}.pth')\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"‚úÖ Entra√Ænement termin√© !\")\n",
    "print(f\"Meilleure val loss: {best_val_loss:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization"
   },
   "source": [
    "## üìä Visualisation des r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_results"
   },
   "outputs": [],
   "source": [
    "print(\"üìä G√©n√©ration des visualisations...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Total loss\n",
    "axes[0].plot(history['train_loss'], label='Train', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val', linewidth=2)\n",
    "axes[0].set_title('Total Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Reconstruction loss\n",
    "axes[1].plot(history['train_recon'], label='Train', linewidth=2)\n",
    "axes[1].plot(history['val_recon'], label='Val', linewidth=2)\n",
    "axes[1].set_title('Reconstruction Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# KL divergence\n",
    "axes[2].plot(history['train_kl'], label='Train', linewidth=2)\n",
    "axes[2].plot(history['val_kl'], label='Val', linewidth=2)\n",
    "axes[2].set_title('KL Divergence')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('KL')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(CONFIG['checkpoint_dir']) / 'training_history.png', dpi=150)\n",
    "print(f\"‚úì Sauvegard√©: {CONFIG['checkpoint_dir']}/training_history.png\")\n",
    "plt.show()\n",
    "\n",
    "# Sauvegarder historique\n",
    "with open(Path(CONFIG['checkpoint_dir']) / 'history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"üéâ TOUT EST PR√äT !\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìÅ Fichiers cr√©√©s:\")\n",
    "print(f\"  {CONFIG['checkpoint_dir']}/vae_best.pth\")\n",
    "print(f\"  {CONFIG['checkpoint_dir']}/training_history.png\")\n",
    "print(f\"  {CONFIG['checkpoint_dir']}/history.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_to_drive"
   },
   "source": [
    "## üíæ Sauvegarde sur Google Drive (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_drive"
   },
   "outputs": [],
   "source": [
    "# D√©commentez pour copier les checkpoints vers Google Drive\n",
    "# import shutil\n",
    "# drive_checkpoint_dir = '/content/drive/MyDrive/vae_checkpoints'\n",
    "# Path(drive_checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "# shutil.copytree(CONFIG['checkpoint_dir'], drive_checkpoint_dir, dirs_exist_ok=True)\n",
    "# print(f\"‚úì Checkpoints copi√©s vers {drive_checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## üöÄ Prochaines √©tapes\n",
    "\n",
    "Maintenant que votre VAE est entra√Æn√©, vous pouvez :\n",
    "\n",
    "1. **G√©n√©rer de nouveaux sc√©narios** en √©chantillonnant dans l'espace latent\n",
    "2. **Analyser l'espace latent** pour comprendre ce que le mod√®le a appris\n",
    "3. **Interpoler** entre diff√©rents sc√©narios\n",
    "4. **Reconstruire** des s√©quences existantes pour √©valuer la qualit√©\n",
    "\n",
    "---\n",
    "\n",
    "**Bon entra√Ænement ! üéâ**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
