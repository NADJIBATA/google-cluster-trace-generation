"""
Pr√©paration des S√©quences pour le VAE
======================================

Ce script prend vos s√©ries temporelles et cr√©e des s√©quences 
pr√™tes pour l'entra√Ænement du VAE.

√âtapes:
1. Charger les donn√©es de s√©ries temporelles
2. Cr√©er des s√©quences avec fen√™tre glissante
3. Normaliser les donn√©es
4. Split train/val/test
5. Sauvegarder tout
"""

import numpy as np
import pandas as pd
import pickle
from pathlib import Path
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import sys

# Ajouter le chemin du module
sys.path.append('.')

print("="*70)
print("üîß PR√âPARATION DES S√âQUENCES POUR LE VAE")
print("="*70)

# ============================================================================
# CONFIGURATION
# ============================================================================

CONFIG = {
    'sequence_length': 288,      # Longueur des s√©quences (288 √ó 5min = 24h)
    'stride': 12,                # Pas de glissement (12 √ó 5min = 1h entre chaque s√©quence)
    'train_ratio': 0.7,          # 70% pour entra√Ænement
    'val_ratio': 0.15,           # 15% pour validation
    'test_ratio': 0.15,          # 15% pour test
    'random_seed': 42,           # Pour reproductibilit√©
    
    # Input file - MODIFIEZ SI N√âCESSAIRE
    # Use relative path (no leading slash) so the script finds files in the repo
    'input_file': 'data/processed/time_series_dt5min.csv',
}

print(f"\n‚öôÔ∏è  Configuration:")
print(f"   Longueur s√©quence: {CONFIG['sequence_length']} timesteps")
print(f"   Dur√©e s√©quence: {CONFIG['sequence_length'] * 5 / 60:.1f} heures (avec Œît=5min)")
print(f"   Stride: {CONFIG['stride']} timesteps ({CONFIG['stride'] * 5} minutes)")
print(f"   Split: {CONFIG['train_ratio']:.0%}/{CONFIG['val_ratio']:.0%}/{CONFIG['test_ratio']:.0%}")

# Estimation du nombre de s√©quences
print(f"\nüí° Avec ces param√®tres:")
print(f"   - stride=12 : nouvelle s√©quence toutes les heures")
print(f"   - Si vous avez 30 jours de donn√©es ‚Üí ~700 s√©quences")
print(f"   - Si vous avez 7 jours  ‚Üí ~170 s√©quences")
print(f"   - Si vous avez 3 mois   ‚Üí ~2100 s√©quences")

# ============================================================================
# √âTAPE 1 : CHARGER LES DONN√âES
# ============================================================================

print(f"\n{'='*70}")
print("üìÇ √âTAPE 1 : Chargement des donn√©es")
print("="*70)

INPUT_FILE = CONFIG['input_file']

try:
    # Charger la s√©rie temporelle
    df_ts = pd.read_csv(INPUT_FILE, index_col=0, parse_dates=True)
    print(f"‚úì Charg√©: {INPUT_FILE}")
    print(f"  Shape: {df_ts.shape}")
    print(f"  Colonnes: {df_ts.columns.tolist()}")
    print(f"  P√©riode: {df_ts.index.min()} ‚Üí {df_ts.index.max()}")
    print(f"  Dur√©e: {(df_ts.index.max() - df_ts.index.min()).days} jours")
    
    # D√©tecter la colonne de donn√©es
    if 'job_count' in df_ts.columns:
        data_col = 'job_count'
    elif 'arrival_rate' in df_ts.columns:
        data_col = 'arrival_rate'
    elif 'num_arrivals' in df_ts.columns:
        data_col = 'num_arrivals'
    else:
        # Prendre la premi√®re colonne num√©rique
        numeric_cols = df_ts.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            data_col = numeric_cols[0]
            print(f"\n‚ö†Ô∏è  Colonne auto-d√©tect√©e: '{data_col}'")
        else:
            raise ValueError(f"Aucune colonne num√©rique trouv√©e. Colonnes: {df_ts.columns.tolist()}")
    
    print(f"\n  Colonne utilis√©e: '{data_col}'")
    arrival_rates = df_ts[data_col].values
    
    print(f"\n  Statistiques {data_col}:")
    print(f"    N points:   {len(arrival_rates):,}")
    print(f"    Min:        {arrival_rates.min():.2f}")
    print(f"    Max:        {arrival_rates.max():.2f}")
    print(f"    Mean:       {arrival_rates.mean():.2f}")
    print(f"    Std:        {arrival_rates.std():.2f}")
    print(f"    M√©diane:    {np.median(arrival_rates):.2f}")
    
    # V√©rifier s'il y a assez de donn√©es
    min_length_needed = CONFIG['sequence_length']
    if len(arrival_rates) < min_length_needed:
        raise ValueError(
            f"Pas assez de donn√©es ! "
            f"Vous avez {len(arrival_rates)} points, "
            f"mais il faut au moins {min_length_needed} pour cr√©er une s√©quence."
        )
    
    # Calculer le nombre de s√©quences qu'on va cr√©er
    n_sequences_expected = (len(arrival_rates) - CONFIG['sequence_length']) // CONFIG['stride'] + 1
    print(f"\n  S√©quences attendues: {n_sequences_expected:,}")
    
    if n_sequences_expected < 100:
        print(f"\n  ‚ö†Ô∏è  ATTENTION : Seulement {n_sequences_expected} s√©quences !")
        print(f"     Recommand√© : au moins 500-1000 s√©quences")
        print(f"     Solutions :")
        print(f"       - R√©duire sequence_length (ex: 144 au lieu de 288)")
        print(f"       - R√©duire stride (ex: 6 au lieu de 12)")
        print(f"       - Utiliser plus de donn√©es")
    
except FileNotFoundError:
    print(f"‚ùå Fichier non trouv√©: {INPUT_FILE}")
    print(f"\nüí° Fichiers disponibles:")
    
    # Chercher des fichiers time_series
    time_series_dir = Path("data/processed/")
    if time_series_dir.exists():
        files = list(time_series_dir.glob("*.csv"))
        if files:
            print(f"   Trouv√©s dans {time_series_dir}:")
            for f in files:
                print(f"     - {f.name}")
            print(f"\n   Modifiez CONFIG['input_file'] avec le bon chemin.")
        else:
            print(f"   Aucun fichier .csv trouv√© dans {time_series_dir}")
    else:
        print(f"   Le dossier {time_series_dir} n'existe pas.")
        print(f"\n   Vous devez d'abord cr√©er la s√©rie temporelle avec:")
        print(f"     python src/data/builder.py")
    
    sys.exit(1)

except Exception as e:
    print(f"‚ùå Erreur : {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# ============================================================================
# √âTAPE 2 : CR√âER LES S√âQUENCES
# ============================================================================

print(f"\n{'='*70}")
print("‚úÇÔ∏è  √âTAPE 2 : Cr√©ation des s√©quences")
print("="*70)

def create_sequences(data, seq_length, stride):
    """
    Cr√©e des s√©quences avec fen√™tre glissante.
    
    Args:
        data: Array 1D des valeurs
        seq_length: Longueur des s√©quences
        stride: Pas de glissement
    
    Returns:
        Array 3D (n_sequences, seq_length, 1)
    """
    if len(data) < seq_length:
        raise ValueError(f"Donn√©es trop courtes! {len(data)} < {seq_length}")
    
    sequences = []
    for i in range(0, len(data) - seq_length + 1, stride):
        seq = data[i:i + seq_length]
        sequences.append(seq)
    
    # Convertir en array 3D
    sequences = np.array(sequences)
    sequences = sequences.reshape(sequences.shape[0], sequences.shape[1], 1)
    
    return sequences

# Cr√©er les s√©quences
print(f"Cr√©ation avec fen√™tre glissante...")
print(f"  Longueur: {CONFIG['sequence_length']}")
print(f"  Stride: {CONFIG['stride']}")

sequences = create_sequences(
    arrival_rates, 
    CONFIG['sequence_length'], 
    CONFIG['stride']
)

print(f"\n‚úì S√©quences cr√©√©es: {sequences.shape}")
print(f"  N s√©quences:  {sequences.shape[0]:,}")
print(f"  Longueur:     {sequences.shape[1]}")
print(f"  N features:   {sequences.shape[2]}")

# V√©rifier la couverture
coverage = (sequences.shape[0] * CONFIG['stride']) / len(arrival_rates) * 100
print(f"  Couverture:   {coverage:.1f}% des donn√©es utilis√©es")

# Alertes
if sequences.shape[0] < 100:
    print(f"\n  ‚ö†Ô∏è  ATTENTION : Seulement {sequences.shape[0]} s√©quences cr√©√©es !")
    print(f"     C'est tr√®s peu pour entra√Æner un VAE.")
    print(f"     Recommandations :")
    print(f"       - R√©duire stride √† 6 ou 1")
    print(f"       - Ou r√©duire sequence_length √† 144")
elif sequences.shape[0] < 500:
    print(f"\n  ‚ÑπÔ∏è  {sequences.shape[0]} s√©quences : c'est peu mais acceptable")
else:
    print(f"\n  ‚úÖ {sequences.shape[0]} s√©quences : bon nombre !")

# Visualiser quelques s√©quences
print(f"\nüìä Visualisation de 5 s√©quences al√©atoires...")

fig, axes = plt.subplots(5, 1, figsize=(14, 10))
np.random.seed(42)
n_samples = min(5, len(sequences))
sample_indices = np.random.choice(len(sequences), n_samples, replace=False)

for idx, seq_idx in enumerate(sample_indices):
    axes[idx].plot(sequences[seq_idx, :, 0], linewidth=1.5)
    axes[idx].set_ylabel('Valeur')
    axes[idx].set_title(f'S√©quence #{seq_idx} (longueur={CONFIG["sequence_length"]})', 
                       fontsize=10, fontweight='bold')
    axes[idx].grid(True, alpha=0.3)
    axes[idx].axhline(y=sequences[seq_idx, :, 0].mean(), 
                     color='red', linestyle='--', alpha=0.5, label='Moyenne')
    if idx == 0:
        axes[idx].legend()

axes[-1].set_xlabel(f'Position dans la s√©quence (0-{CONFIG["sequence_length"]-1})')
plt.tight_layout()

output_dir_temp = Path("data/processed/sequences")
output_dir_temp.mkdir(parents=True, exist_ok=True)
plt.savefig(output_dir_temp / 'sample_sequences.png', dpi=150)
print(f"‚úì Sauvegard√©: {output_dir_temp / 'sample_sequences.png'}")
plt.close()

# ============================================================================
# √âTAPE 3 : NORMALISATION
# ============================================================================

print(f"\n{'='*70}")
print("üìè √âTAPE 3 : Normalisation")
print("="*70)

# Cr√©er le scaler
scaler = StandardScaler()

# Reshape pour scaler: (n_sequences * seq_length, n_features)
n_sequences, seq_length, n_features = sequences.shape
sequences_flat = sequences.reshape(-1, n_features)

print(f"Avant normalisation:")
print(f"  Shape: {sequences_flat.shape}")
print(f"  Mean:  {sequences_flat.mean():.4f}")
print(f"  Std:   {sequences_flat.std():.4f}")
print(f"  Min:   {sequences_flat.min():.4f}")
print(f"  Max:   {sequences_flat.max():.4f}")

# Fit et transform
sequences_norm_flat = scaler.fit_transform(sequences_flat)

# Reshape back
sequences_norm = sequences_norm_flat.reshape(n_sequences, seq_length, n_features)

print(f"\nApr√®s normalisation:")
print(f"  Mean:  {sequences_norm_flat.mean():.6f} (devrait √™tre ~0)")
print(f"  Std:   {sequences_norm_flat.std():.6f} (devrait √™tre ~1)")
print(f"  Min:   {sequences_norm_flat.min():.4f}")
print(f"  Max:   {sequences_norm_flat.max():.4f}")

# ============================================================================
# √âTAPE 4 : SPLIT TRAIN/VAL/TEST
# ============================================================================

print(f"\n{'='*70}")
print("üîÄ √âTAPE 4 : Split train/val/test")
print("="*70)

# Shuffle puis split
np.random.seed(CONFIG['random_seed'])
indices = np.random.permutation(len(sequences_norm))

# Calculer indices
n_total = len(sequences_norm)
n_train = int(n_total * CONFIG['train_ratio'])
n_val = int(n_total * CONFIG['val_ratio'])

# Split
train_indices = indices[:n_train]
val_indices = indices[n_train:n_train + n_val]
test_indices = indices[n_train + n_val:]

train_sequences = sequences_norm[train_indices]
val_sequences = sequences_norm[val_indices]
test_sequences = sequences_norm[test_indices]

print(f"‚úì Split effectu√©:")
print(f"  Train: {train_sequences.shape} ({len(train_sequences)/n_total*100:.1f}%)")
print(f"  Val:   {val_sequences.shape} ({len(val_sequences)/n_total*100:.1f}%)")
print(f"  Test:  {test_sequences.shape} ({len(test_sequences)/n_total*100:.1f}%)")

# V√©rification finale
if len(train_sequences) < 50:
    print(f"\n  ‚ö†Ô∏è  WARNING : Seulement {len(train_sequences)} √©chantillons d'entra√Ænement !")
    print(f"     Le VAE risque de ne pas bien apprendre.")

# ============================================================================
# √âTAPE 5 : SAUVEGARDE
# ============================================================================

print(f"\n{'='*70}")
print("üíæ √âTAPE 5 : Sauvegarde")
print("="*70)

# Cr√©er dossier de sortie
output_dir = Path("data/processed/sequences")
output_dir.mkdir(parents=True, exist_ok=True)

# Sauvegarder les s√©quences
np.save(output_dir / 'train.npy', train_sequences)
np.save(output_dir / 'val.npy', val_sequences)
np.save(output_dir / 'test.npy', test_sequences)

print(f"‚úì S√©quences sauvegard√©es:")
print(f"  {output_dir / 'train.npy'}")
print(f"  {output_dir / 'val.npy'}")
print(f"  {output_dir / 'test.npy'}")

# Sauvegarder le scaler
with open(output_dir / 'scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

print(f"‚úì Scaler sauvegard√©: {output_dir / 'scaler.pkl'}")

# Sauvegarder la config
import json
config_to_save = CONFIG.copy()
config_to_save['n_sequences'] = int(n_total)
config_to_save['n_train'] = int(len(train_sequences))
config_to_save['n_val'] = int(len(val_sequences)s)
config_to_save['n_test'] = int(len(test_sequences))
config_to_save['data_source'] = str(INPUT_FILE)
config_to_save['n_features'] = int(n_features)

with open(output_dir / 'config.json', 'w') as f:
    json.dump(config_to_save, f, indent=2)

print(f"‚úì Configuration sauvegard√©e: {output_dir / 'config.json'}")

# ============================================================================
# √âTAPE 6 : VALIDATION
# ============================================================================

print(f"\n{'='*70}")
print("‚úÖ √âTAPE 6 : Validation")
print("="*70)

# Tester le chargement
train_loaded = np.load(output_dir / 'train.npy')
print(f"‚úì Test de chargement: {train_loaded.shape}")

# V√©rifier la d√©normalisation
sample_seq_norm = train_sequences[0]  # (seq_length, 1)
sample_seq_denorm = scaler.inverse_transform(sample_seq_norm)

print(f"\n‚úì Test de d√©normalisation:")
print(f"  Normalis√©:    min={sample_seq_norm.min():.2f}, max={sample_seq_norm.max():.2f}")
print(f"  D√©normalis√©:  min={sample_seq_denorm.min():.2f}, max={sample_seq_denorm.max():.2f}")

# Visualiser distribution
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Distribution avant normalisation
axes[0].hist(sequences.flatten(), bins=50, alpha=0.7, edgecolor='black')
axes[0].set_title('Distribution Avant Normalisation', fontweight='bold')
axes[0].set_xlabel(data_col)
axes[0].set_ylabel('Fr√©quence')
axes[0].grid(True, alpha=0.3)
axes[0].axvline(sequences.mean(), color='red', linestyle='--', 
               linewidth=2, label=f'Mean={sequences.mean():.1f}')
axes[0].legend()

# Distribution apr√®s normalisation
axes[1].hist(sequences_norm.flatten(), bins=50, alpha=0.7, edgecolor='black')
axes[1].set_title('Distribution Apr√®s Normalisation (N(0,1))', fontweight='bold')
axes[1].set_xlabel('Valeur Normalis√©e')
axes[1].set_ylabel('Fr√©quence')
axes[1].grid(True, alpha=0.3)
axes[1].axvline(0, color='red', linestyle='--', linewidth=2, label='Mean=0')
axes[1].legend()

plt.tight_layout()
plt.savefig(output_dir / 'normalization_check.png', dpi=150)
print(f"\n‚úì Visualisation sauvegard√©e: {output_dir / 'normalization_check.png'}")
plt.close()

# ============================================================================
# R√âSUM√â FINAL
# ============================================================================

print(f"\n{'='*70}")
print("üéâ PR√âPARATION TERMIN√âE AVEC SUCC√àS !")
print("="*70)

print(f"\nüìä R√©sum√©:")
print(f"  Donn√©es sources:     {len(arrival_rates):,} timesteps")
print(f"  P√©riode:             {(df_ts.index.max() - df_ts.index.min()).days} jours")
print(f"  S√©quences cr√©√©es:    {n_total:,} s√©quences de longueur {CONFIG['sequence_length']}")
print(f"  Train:               {len(train_sequences):,} s√©quences")
print(f"  Validation:          {len(val_sequences):,} s√©quences")
print(f"  Test:                {len(test_sequences):,} s√©quences")

print(f"\nüìÅ Fichiers cr√©√©s dans {output_dir}:")
print(f"  ‚úì train.npy         {train_sequences.shape}")
print(f"  ‚úì val.npy           {val_sequences.shape}")
print(f"  ‚úì test.npy          {test_sequences.shape}")
print(f"  ‚úì scaler.pkl")
print(f"  ‚úì config.json")
print(f"  ‚úì sample_sequences.png")
print(f"  ‚úì normalization_check.png")

print(f"\nüöÄ Prochaine √©tape:")
if len(train_sequences) >= 500:
    print(f"  ‚úÖ Vous avez assez de donn√©es !")
    print(f"  Lancez: python scripts/train_lstm_vae.py")
elif len(train_sequences) >= 100:
    print(f"  ‚ö†Ô∏è  Vous avez peu de donn√©es ({len(train_sequences)} s√©quences)")
    print(f"  Vous pouvez quand m√™me essayer:")
    print(f"    python scripts/train_lstm_vae.py")
    print(f"  Mais pour de meilleurs r√©sultats, r√©duisez stride ou augmentez les donn√©es.")
else:
    print(f"  ‚ùå Pas assez de donn√©es ({len(train_sequences)} s√©quences)")
    print(f"  Recommandations:")
    print(f"    - R√©duire stride (ex: stride=6)")
    print(f"    - R√©duire sequence_length (ex: sequence_length=144)")
    print(f"    - Utiliser plus de donn√©es sources")

print(f"\n{'='*70}")