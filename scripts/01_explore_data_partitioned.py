#!/usr/bin/env python3
"""
Script d'exploration des donn√©es Google Cluster 2011 - VERSION PARTITIONN√âE
Adapt√© pour g√©rer les fichiers part-*-of-00500.csv.gz
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime, timedelta
import json
import glob
from tqdm import tqdm

# Configuration - MODIFIEZ LE CHEMIN ICI
DATA_PATH = Path("data/processed")  # ‚ö†Ô∏è CHANGEZ CECI selon o√π sont vos donn√©es
OUTPUT_PATH = Path("/data/processed")
FIGURE_PATH = Path("/mnt/user-data/outputs/results/figures")

OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
FIGURE_PATH.mkdir(parents=True, exist_ok=True)

def find_partition_files(data_path):
    """Trouve tous les fichiers part-*-of-*.csv.gz"""
    print(f"üîç Recherche des fichiers partitionn√©s dans {data_path}")
    
    patterns = [
        "part-*-of-*.csv.gz",
        "part-*.csv.gz",
        "*.csv.gz"
    ]
    
    files = []
    for pattern in patterns:
        found = list(data_path.glob(pattern))
        files.extend(found)
    
    # Trier par nom pour traiter dans l'ordre
    files = sorted(set(files))
    
    print(f"‚úÖ Trouv√© {len(files)} fichiers")
    
    if files:
        total_size = sum(f.stat().st_size for f in files) / (1024**3)  # GB
        print(f"üìä Taille totale : {total_size:.2f} GB")
    
    return files

def load_single_partition(file_path, nrows=None):
    """Charge un fichier partition."""
    try:
        df = pd.read_csv(file_path, compression='gzip', nrows=nrows)
        return df
    except Exception as e:
        print(f"   ‚ö†Ô∏è Erreur sur {file_path.name}: {e}")
        return None

def identify_columns(df):
    """Identifie automatiquement les colonnes importantes."""
    
    print("\nüîç Identification des colonnes...")
    print(f"   Colonnes disponibles : {df.columns.tolist()}")
    
    # Chercher la colonne event_type
    event_col = None
    for col in df.columns:
        col_lower = col.lower()
        if 'event' in col_lower and 'type' in col_lower:
            event_col = col
            break
    
    if event_col is None:
        # Essayer des noms standards Google Cluster
        possible_names = ['type', 'event_type', 'event', 'event_name']
        for name in possible_names:
            if name in df.columns:
                event_col = name
                break
    
    # Chercher la colonne timestamp
    timestamp_col = None
    for col in df.columns:
        col_lower = col.lower()
        if 'time' in col_lower:
            timestamp_col = col
            break
    
    # Chercher la colonne job_id
    job_col = None
    for col in df.columns:
        col_lower = col.lower()
        if 'job' in col_lower and 'id' in col_lower:
            job_col = col
            break
    
    print(f"\n‚úÖ Colonnes identifi√©es :")
    print(f"   Event Type : {event_col}")
    print(f"   Timestamp  : {timestamp_col}")
    print(f"   Job ID     : {job_col}")
    
    return event_col, timestamp_col, job_col

def load_and_filter_partitions(files, max_files=None, sample_size=None):
    """
    Charge les partitions et filtre les √©v√©nements SUBMIT.
    
    Args:
        files: liste des fichiers √† charger
        max_files: nombre maximum de fichiers √† traiter (None = tous)
        sample_size: nombre de lignes par fichier (None = tout)
    """
    
    print(f"\nüìÇ Chargement des partitions...")
    
    if max_files is not None:
        files = files[:max_files]
        print(f"   Traitement de {len(files)} fichiers (max_files={max_files})")
    else:
        print(f"   Traitement de TOUS les {len(files)} fichiers")
    
    if sample_size is not None:
        print(f"   ‚ö†Ô∏è  Mode √©chantillonnage : {sample_size} lignes par fichier")
    
    all_submit_events = []
    
    # Charger le premier fichier pour identifier les colonnes
    print(f"\nüîç Analyse du premier fichier...")
    first_df = load_single_partition(files[0], nrows=1000)
    if first_df is None:
        print("‚ùå Impossible de charger le premier fichier")
        return None, None, None, None
    
    event_col, timestamp_col, job_col = identify_columns(first_df)
    
    if event_col is None or timestamp_col is None:
        print("‚ùå Colonnes essentielles non trouv√©es !")
        print(f"   Colonnes disponibles : {first_df.columns.tolist()}")
        return None, None, None, None
    
    # Identifier le code pour SUBMIT
    print(f"\nüîç D√©tection du code SUBMIT...")
    event_types = first_df[event_col].unique()
    print(f"   Types d'√©v√©nements trouv√©s : {event_types}")
    
    # SUBMIT peut √™tre :
    # - 0 dans Google Cluster 2011
    # - "SUBMIT" en texte
    # - autre selon la version
    submit_code = None
    if 0 in event_types:
        submit_code = 0
        print(f"   ‚úÖ Code SUBMIT identifi√© : 0")
    elif 'SUBMIT' in [str(e).upper() for e in event_types]:
        submit_code = 'SUBMIT'
        print(f"   ‚úÖ Code SUBMIT identifi√© : 'SUBMIT'")
    else:
        print(f"   ‚ö†Ô∏è  Code SUBMIT non identifi√© automatiquement")
        print(f"   Types trouv√©s : {event_types}")
        submit_code = event_types[0]  # Par d√©faut, prendre le premier
        print(f"   ‚ö†Ô∏è  Utilisation de {submit_code} par d√©faut")
    
    # Maintenant charger tous les fichiers
    print(f"\nüìä Chargement et filtrage de {len(files)} fichiers...")
    
    for file in tqdm(files, desc="Traitement"):
        df = load_single_partition(file, nrows=sample_size)
        
        if df is None:
            continue
        
        # Filtrer les SUBMIT
        if submit_code == 0:
            submit_mask = df[event_col] == 0
        elif submit_code == 'SUBMIT':
            submit_mask = df[event_col].astype(str).str.upper() == 'SUBMIT'
        else:
            submit_mask = df[event_col] == submit_code
        
        df_submit = df[submit_mask].copy()
        
        if len(df_submit) > 0:
            # Extraire timestamp et job_id
            df_submit['timestamp_us'] = pd.to_numeric(df_submit[timestamp_col], errors='coerce')
            
            if job_col is not None:
                df_submit['job_id'] = df_submit[job_col]
            
            all_submit_events.append(df_submit[['timestamp_us', 'job_id'] if job_col else ['timestamp_us']])
    
    if not all_submit_events:
        print("‚ùå Aucun √©v√©nement SUBMIT trouv√© !")
        return None, None, None, None
    
    # Concat√©ner tous les √©v√©nements
    print(f"\nüîó Fusion des donn√©es...")
    df_all_submit = pd.concat(all_submit_events, ignore_index=True)
    
    # Supprimer les doublons potentiels
    df_all_submit = df_all_submit.drop_duplicates()
    
    # Trier par timestamp
    df_all_submit = df_all_submit.sort_values('timestamp_us').reset_index(drop=True)
    
    print(f"‚úÖ {len(df_all_submit):,} √©v√©nements SUBMIT extraits")
    
    return df_all_submit, event_col, timestamp_col, job_col

def create_time_series(df_submit, delta_t_minutes=10):
    """Cr√©e une s√©rie temporelle du nombre d'arriv√©es par intervalle."""
    
    print(f"\n‚è±Ô∏è  Cr√©ation de la s√©rie temporelle (Œît = {delta_t_minutes} minutes)...")
    
    # Convertir en datetime
    df_submit['datetime'] = pd.to_datetime(df_submit['timestamp_us'], unit='us')
    
    # Info sur la p√©riode couverte
    start_time = df_submit['datetime'].min()
    end_time = df_submit['datetime'].max()
    duration = end_time - start_time
    
    print(f"   üìÖ P√©riode : {start_time} ‚Üí {end_time}")
    print(f"   ‚è≥ Dur√©e : {duration.days} jours, {duration.seconds//3600} heures")
    
    # Cr√©er des bins temporels
    df_submit['time_bin'] = df_submit['datetime'].dt.floor(f'{delta_t_minutes}min')
    
    # Compter les arriv√©es par bin
    time_series = df_submit.groupby('time_bin').size().reset_index(name='num_arrivals')
    
    print(f"‚úÖ S√©rie temporelle cr√©√©e : {len(time_series):,} intervalles")
    
    return time_series

def analyze_daily_patterns(time_series):
    """Analyse les patterns journaliers."""
    
    time_series['hour'] = time_series['time_bin'].dt.hour
    time_series['day'] = time_series['time_bin'].dt.date
    time_series['day_of_week'] = time_series['time_bin'].dt.dayofweek
    
    # Statistiques globales
    stats = {
        'mean': float(time_series['num_arrivals'].mean()),
        'std': float(time_series['num_arrivals'].std()),
        'min': int(time_series['num_arrivals'].min()),
        'max': int(time_series['num_arrivals'].max()),
        'median': float(time_series['num_arrivals'].median()),
        'total_jobs': int(time_series['num_arrivals'].sum()),
        'num_intervals': int(len(time_series)),
        'start_time': str(time_series['time_bin'].min()),
        'end_time': str(time_series['time_bin'].max()),
    }
    
    print("\n" + "=" * 70)
    print("üìä STATISTIQUES GLOBALES")
    print("=" * 70)
    for key, value in stats.items():
        if isinstance(value, float):
            print(f"   {key:20s} : {value:,.2f}")
        else:
            print(f"   {key:20s} : {value:,}")
    
    # Patterns horaires
    hourly_pattern = time_series.groupby('hour')['num_arrivals'].agg(['mean', 'std', 'min', 'max'])
    
    print("\nüìà PATTERN HORAIRE MOYEN")
    print(hourly_pattern)
    
    return stats, hourly_pattern

def create_visualizations(time_series, hourly_pattern):
    """Cr√©e des visualisations."""
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 10))
    
    # 1. S√©rie temporelle compl√®te (sous-√©chantillonn√©e si trop long)
    ax = axes[0, 0]
    if len(time_series) > 10000:
        # Sous-√©chantillonner pour la visualisation
        step = len(time_series) // 10000
        ts_plot = time_series.iloc[::step]
        title = f'S√©rie Temporelle - Arriv√©es de Jobs (1/{step} points)'
    else:
        ts_plot = time_series
        title = 'S√©rie Temporelle Compl√®te - Arriv√©es de Jobs'
    
    ax.plot(ts_plot['time_bin'], ts_plot['num_arrivals'], linewidth=0.5)
    ax.set_title(title, fontsize=12, fontweight='bold')
    ax.set_xlabel('Temps')
    ax.set_ylabel('Nombre d\'arriv√©es')
    ax.grid(True, alpha=0.3)
    
    # 2. Distribution des arriv√©es
    ax = axes[0, 1]
    ax.hist(time_series['num_arrivals'], bins=50, edgecolor='black', alpha=0.7)
    ax.set_title('Distribution du Nombre d\'Arriv√©es', fontsize=12, fontweight='bold')
    ax.set_xlabel('Nombre d\'arriv√©es par intervalle')
    ax.set_ylabel('Fr√©quence')
    ax.grid(True, alpha=0.3)
    
    # 3. Pattern horaire
    ax = axes[1, 0]
    ax.plot(hourly_pattern.index, hourly_pattern['mean'], marker='o', linewidth=2)
    ax.fill_between(hourly_pattern.index, 
                     hourly_pattern['mean'] - hourly_pattern['std'],
                     hourly_pattern['mean'] + hourly_pattern['std'],
                     alpha=0.3)
    ax.set_title('Pattern Horaire Moyen (¬± 1 œÉ)', fontsize=12, fontweight='bold')
    ax.set_xlabel('Heure de la journ√©e')
    ax.set_ylabel('Nombre moyen d\'arriv√©es')
    ax.set_xticks(range(0, 24, 2))
    ax.grid(True, alpha=0.3)
    
    # 4. Boxplot par jour de la semaine
    ax = axes[1, 1]
    day_names = ['Lun', 'Mar', 'Mer', 'Jeu', 'Ven', 'Sam', 'Dim']
    time_series['day_name'] = time_series['day_of_week'].map(lambda x: day_names[x])
    time_series.boxplot(column='num_arrivals', by='day_name', ax=ax)
    ax.set_title('Distribution par Jour de la Semaine', fontsize=12, fontweight='bold')
    ax.set_xlabel('Jour')
    ax.set_ylabel('Nombre d\'arriv√©es')
    plt.suptitle('')  # Enlever le titre automatique
    
    plt.tight_layout()
    
    output_file = FIGURE_PATH / '01_exploration_initiale.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"\nüíæ Figure sauvegard√©e : {output_file}")
    
    plt.close()

def main():
    """Fonction principale."""
    
    print("=" * 70)
    print("üìä EXPLORATION DES DONN√âES GOOGLE CLUSTER 2011 (PARTITIONN√âES)")
    print("=" * 70)
    
    # Configuration
    MAX_FILES = None  # Mettre un nombre pour tester (ex: 10), None pour tout
    SAMPLE_SIZE = None  # Mettre un nombre pour tester (ex: 100000), None pour tout
    DELTA_T = 10  # minutes
    
    if MAX_FILES is not None:
        print(f"\n‚ö†Ô∏è  MODE TEST : Traitement de {MAX_FILES} fichiers seulement")
    if SAMPLE_SIZE is not None:
        print(f"‚ö†Ô∏è  MODE √âCHANTILLONNAGE : {SAMPLE_SIZE} lignes par fichier")
    
    # 1. Trouver les fichiers
    print(f"\nüìÅ Recherche dans : {DATA_PATH}")
    files = find_partition_files(DATA_PATH)
    
    if not files:
        print("\n‚ùå Aucun fichier trouv√© !")
        print("\nüí° Solutions :")
        print("   1. V√©rifiez que DATA_PATH est correct (ligne 18 du script)")
        print("   2. V√©rifiez que les fichiers sont bien au format part-*-of-*.csv.gz")
        print("   3. Listez le contenu : ls /data/raw/")
        return
    
    # 2. Charger et filtrer
    df_submit, event_col, timestamp_col, job_col = load_and_filter_partitions(
        files, 
        max_files=MAX_FILES,
        sample_size=SAMPLE_SIZE
    )
    
    if df_submit is None:
        print("‚ùå √âchec du chargement")
        return
    
    # 3. Cr√©er la s√©rie temporelle
    time_series = create_time_series(df_submit, delta_t_minutes=DELTA_T)
    
    # 4. Analyser les patterns
    stats, hourly_pattern = analyze_daily_patterns(time_series)
    
    # 5. Visualiser
    print("\nüìä Cr√©ation des visualisations...")
    create_visualizations(time_series, hourly_pattern)
    
    # 6. Sauvegarder
    output_file = OUTPUT_PATH / f'time_series_dt{DELTA_T}min.csv'
    time_series.to_csv(output_file, index=False)
    print(f"üíæ S√©rie temporelle sauvegard√©e : {output_file}")
    
    stats_file = OUTPUT_PATH / 'stats_exploration.json'
    with open(stats_file, 'w') as f:
        json.dump(stats, f, indent=2, default=str)
    print(f"üíæ Statistiques sauvegard√©es : {stats_file}")
    
    print("\n" + "=" * 70)
    print("‚úÖ EXPLORATION TERMIN√âE")
    print("=" * 70)
    print("\nüìã Prochaines √©tapes :")
    print("   1. Examiner les r√©sultats dans results/figures/")
    print("   2. Si satisfait, relancer avec MAX_FILES=None pour tout traiter")
    print("   3. Ajuster Œît si n√©cessaire")
    print("   4. Lancer : python scripts/02_preprocess_vae.py")
    print("=" * 70)

if __name__ == "__main__":
    main()