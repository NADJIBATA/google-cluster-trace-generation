"""
Test et √âvaluation du LSTM-VAE
================================

Ce script teste le mod√®le entra√Æn√© et diagnostique les probl√®mes potentiels.

Tests effectu√©s:
1. Reconstruction des donn√©es de test
2. Diversit√© de l'espace latent
3. Qualit√© des g√©n√©rations
4. Diagnostic du posterior collapse
5. M√©triques de performance
"""

import numpy as np
import torch
import pickle
from pathlib import Path
import matplotlib.pyplot as plt
import json
import sys
from scipy import stats

sys.path.append('.')

from src.models.vae_lstm import LSTMVAE

print("="*70)
print("üß™ TEST ET √âVALUATION DU LSTM-VAE")
print("="*70)

# ============================================================================
# CONFIGURATION
# ============================================================================

CONFIG = {
    'checkpoint_path': 'checkpoints/lstm_vae_best.pth',
    'test_data_path': 'data/processed/sequences/test.npy',
    'scaler_path': 'data/processed/sequences/scaler.pkl',
    'output_dir': 'results/evaluation',
    'device': 'cuda' if torch.cuda.is_available() else 'cpu'
}

print(f"\n‚öôÔ∏è  Configuration:")
print(f"   Device: {CONFIG['device']}")
print(f"   Checkpoint: {CONFIG['checkpoint_path']}")

# Cr√©er dossier de sortie
output_dir = Path(CONFIG['output_dir'])
output_dir.mkdir(parents=True, exist_ok=True)

# ============================================================================
# TEST 1: CHARGEMENT DU MOD√àLE
# ============================================================================

print(f"\n{'='*70}")
print("üìÇ TEST 1: Chargement du mod√®le")
print("="*70)

checkpoint_path = Path(CONFIG['checkpoint_path'])
if not checkpoint_path.exists():
    print(f"‚ùå Checkpoint non trouv√©: {checkpoint_path}")
    print(f"\nFichiers disponibles dans checkpoints/:")
    if Path('checkpoints').exists():
        for f in Path('checkpoints').glob('*.pth'):
            print(f"   - {f.name}")
    sys.exit(1)

checkpoint = torch.load(checkpoint_path, map_location=CONFIG['device'])

print(f"‚úì Checkpoint charg√©:")
print(f"   Epoch: {checkpoint['epoch']}")
print(f"   Val loss: {checkpoint['val_loss']:.4f}")

# Reconstruire le mod√®le
model_config = checkpoint['vae_config']
model = LSTMVAE(
    input_size=1,
    sequence_length=checkpoint['config']['sequence_length'],
    **model_config
)

model.load_state_dict(checkpoint['model_state_dict'])
model = model.to(CONFIG['device'])
model.eval()

print(f"\n‚úì Mod√®le reconstruit:")
print(f"   Param√®tres: {model.count_parameters():,}")
print(f"   Latent dim: {model_config['latent_dim']}")

# ============================================================================
# TEST 2: DIAGNOSTIC DU POSTERIOR COLLAPSE
# ============================================================================

print(f"\n{'='*70}")
print("üîç TEST 2: Diagnostic du Posterior Collapse")
print("="*70)

# Analyser l'historique
if 'history' in checkpoint:
    history = checkpoint['history']
    
    final_kl = history['val_kl'][-1] if isinstance(history['val_kl'], list) else history['val_kl']
    final_recon = history['val_recon'][-1] if isinstance(history['val_recon'], list) else history['val_recon']
    
    print(f"\nüìä M√©triques finales:")
    print(f"   KL divergence: {final_kl:.4f}")
    print(f"   Reconstruction: {final_recon:.2f}")
    
    if final_kl > 0:
        ratio = final_recon / final_kl
        print(f"   Ratio R/KL: {ratio:.2f}")
    
    # Diagnostic
    print(f"\nüîç Diagnostic:")
    if final_kl < 0.01:
        print(f"   ‚ùå SEVERE COLLAPSE: KL ‚âà 0")
        print(f"      Le mod√®le ignore compl√®tement l'espace latent !")
        collapse_severity = "SEVERE"
    elif final_kl < 0.1:
        print(f"   ‚ùå POSTERIOR COLLAPSE: KL tr√®s faible ({final_kl:.4f})")
        print(f"      Le mod√®le utilise tr√®s peu l'espace latent")
        collapse_severity = "HIGH"
    elif final_kl < 1.0:
        print(f"   ‚ö†Ô∏è  RISQUE DE COLLAPSE: KL faible ({final_kl:.2f})")
        print(f"      Surveillez la diversit√© des g√©n√©rations")
        collapse_severity = "MEDIUM"
    elif final_kl > 20:
        print(f"   ‚ö†Ô∏è  KL TR√àS √âLEV√âE: {final_kl:.2f}")
        print(f"      Sur-r√©gularisation possible")
        collapse_severity = "OVER_REGULARIZED"
    else:
        print(f"   ‚úÖ KL SAINE: {final_kl:.2f}")
        print(f"      L'espace latent est bien utilis√©")
        collapse_severity = "NONE"
else:
    print("‚ö†Ô∏è  Historique non disponible dans le checkpoint")
    collapse_severity = "UNKNOWN"

# ============================================================================
# TEST 3: RECONSTRUCTION SUR DONN√âES DE TEST
# ============================================================================

print(f"\n{'='*70}")
print("üîÑ TEST 3: Reconstruction sur donn√©es de test")
print("="*70)

test_path = Path(CONFIG['test_data_path'])
if test_path.exists():
    test_data = np.load(test_path)
    print(f"‚úì Donn√©es de test charg√©es: {test_data.shape}")
    
    # Prendre quelques √©chantillons
    n_samples = min(10, len(test_data))
    test_samples = test_data[:n_samples]
    
    # Reconstruction
    with torch.no_grad():
        x_test = torch.FloatTensor(test_samples).to(CONFIG['device'])
        x_recon, mu, logvar = model(x_test)
        
        x_recon = x_recon.cpu().numpy()
        mu = mu.cpu().numpy()
        logvar = logvar.cpu().numpy()
    
    # Calculer MSE
    mse = np.mean((test_samples - x_recon) ** 2)
    print(f"\nüìä Erreur de reconstruction:")
    print(f"   MSE: {mse:.4f}")
    print(f"   RMSE: {np.sqrt(mse):.4f}")
    
    # Visualiser quelques reconstructions
    fig, axes = plt.subplots(min(3, n_samples), 1, figsize=(14, 3*min(3, n_samples)))
    if n_samples == 1:
        axes = [axes]
    
    for i in range(min(3, n_samples)):
        ax = axes[i] if n_samples > 1 else axes[0]
        
        ax.plot(test_samples[i].flatten(), label='Original', linewidth=2, alpha=0.7)
        ax.plot(x_recon[i].flatten(), label='Reconstruction', linewidth=2, alpha=0.7, linestyle='--')
        ax.set_title(f'√âchantillon {i+1}', fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'reconstruction_test.png', dpi=150)
    print(f"\n‚úì Visualisation sauvegard√©e: {output_dir / 'reconstruction_test.png'}")
    plt.close()
    
else:
    print(f"‚ö†Ô∏è  Donn√©es de test non trouv√©es: {test_path}")
    test_data = None

# ============================================================================
# TEST 4: DIVERSIT√â DE L'ESPACE LATENT
# ============================================================================

print(f"\n{'='*70}")
print("üé® TEST 4: Diversit√© de l'espace latent")
print("="*70)

# Charger donn√©es d'entra√Ænement pour analyser l'espace latent
train_path = Path('data/processed/sequences/train.npy')
if train_path.exists():
    train_data = np.load(train_path)
    print(f"‚úì Donn√©es d'entra√Ænement charg√©es: {train_data.shape}")
    
    # Encoder toutes les s√©quences
    print(f"Encodage de {len(train_data)} s√©quences...")
    latent_codes = []
    
    with torch.no_grad():
        batch_size = 32
        for i in range(0, len(train_data), batch_size):
            batch = train_data[i:i+batch_size]
            batch_tensor = torch.FloatTensor(batch).to(CONFIG['device'])
            mu, logvar = model.encode(batch_tensor)
            latent_codes.append(mu.cpu().numpy())
    
    latent_codes = np.concatenate(latent_codes, axis=0)
    print(f"‚úì Espace latent: {latent_codes.shape}")
    
    # Statistiques
    latent_mean = latent_codes.mean(axis=0)
    latent_std = latent_codes.std(axis=0)
    latent_var = latent_codes.var(axis=0)
    
    print(f"\nüìä Statistiques de l'espace latent:")
    print(f"   Mean (global): {latent_mean.mean():.4f}")
    print(f"   Std (global): {latent_std.mean():.4f}")
    print(f"   Variance (avg): {latent_var.mean():.4f}")
    print(f"   Variance (min): {latent_var.min():.4f}")
    print(f"   Variance (max): {latent_var.max():.4f}")
    
    # Diagnostic de collapse bas√© sur variance
    inactive_dims = np.sum(latent_var < 0.01)
    print(f"\nüîç Dimensions inactives (var < 0.01): {inactive_dims}/{len(latent_var)}")
    
    if inactive_dims > len(latent_var) * 0.5:
        print(f"   ‚ùå Plus de 50% des dimensions sont inactives !")
        print(f"      ‚Üí Posterior collapse confirm√©")
    elif inactive_dims > len(latent_var) * 0.2:
        print(f"   ‚ö†Ô∏è  {inactive_dims} dimensions inactives")
        print(f"      ‚Üí Sous-utilisation de l'espace latent")
    else:
        print(f"   ‚úÖ Espace latent bien utilis√©")
    
    # Visualiser variance par dimension
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Variance
    ax = axes[0]
    ax.bar(range(len(latent_var)), latent_var)
    ax.set_title('Variance par Dimension Latente', fontweight='bold')
    ax.set_xlabel('Dimension')
    ax.set_ylabel('Variance')
    ax.axhline(y=0.01, color='red', linestyle='--', alpha=0.5, label='Seuil inactif')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # Distribution de la premi√®re dimension
    ax = axes[1]
    ax.hist(latent_codes[:, 0], bins=50, edgecolor='black', alpha=0.7)
    ax.set_title('Distribution - Dimension 0', fontweight='bold')
    ax.set_xlabel('Valeur')
    ax.set_ylabel('Fr√©quence')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'latent_space_analysis.png', dpi=150)
    print(f"\n‚úì Analyse sauvegard√©e: {output_dir / 'latent_space_analysis.png'}")
    plt.close()
    
else:
    print(f"‚ö†Ô∏è  Donn√©es d'entra√Ænement non trouv√©es")
    latent_codes = None

# ============================================================================
# TEST 5: QUALIT√â DES G√âN√âRATIONS
# ============================================================================

print(f"\n{'='*70}")
print("üé≤ TEST 5: Qualit√© des g√©n√©rations")
print("="*70)

# G√©n√©rer plusieurs √©chantillons
n_gen = 20
print(f"G√©n√©ration de {n_gen} √©chantillons...")

generated_samples = []
with torch.no_grad():
    for i in range(n_gen):
        z = torch.randn(1, model.latent_dim).to(CONFIG['device'])
        x_gen = model.decode(z)
        generated_samples.append(x_gen.cpu().numpy()[0])

generated_samples = np.array(generated_samples)
print(f"‚úì G√©n√©r√©: {generated_samples.shape}")

# Statistiques
gen_mean = generated_samples.mean()
gen_std = generated_samples.std()
gen_min = generated_samples.min()
gen_max = generated_samples.max()

print(f"\nüìä Statistiques des g√©n√©rations (normalis√©es):")
print(f"   Mean: {gen_mean:.4f}")
print(f"   Std:  {gen_std:.4f}")
print(f"   Min:  {gen_min:.4f}")
print(f"   Max:  {gen_max:.4f}")

# V√©rifier diversit√©
unique_values = np.unique(generated_samples.round(2))
print(f"\nüé® Diversit√©:")
print(f"   Valeurs uniques: {len(unique_values)}")

if len(unique_values) < 10:
    print(f"   ‚ùå TR√àS PEU DE DIVERSIT√â !")
    print(f"      Les g√©n√©rations sont presque identiques")
    diversity = "LOW"
elif len(unique_values) < 50:
    print(f"   ‚ö†Ô∏è  Diversit√© limit√©e")
    diversity = "MEDIUM"
else:
    print(f"   ‚úÖ Bonne diversit√©")
    diversity = "HIGH"

# Calculer la variance inter-√©chantillons
inter_sample_var = np.var([s.mean() for s in generated_samples])
print(f"   Variance inter-√©chantillons: {inter_sample_var:.4f}")

if inter_sample_var < 0.01:
    print(f"   ‚ùå Tous les √©chantillons sont quasi-identiques !")
elif inter_sample_var < 0.1:
    print(f"   ‚ö†Ô∏è  Faible variation entre √©chantillons")
else:
    print(f"   ‚úÖ Variation satisfaisante")

# Visualiser
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Quelques g√©n√©rations
ax = axes[0]
for i in range(min(5, n_gen)):
    ax.plot(generated_samples[i].flatten(), alpha=0.7, label=f'Gen {i+1}')
ax.set_title('√âchantillons G√©n√©r√©s (normalis√©s)', fontweight='bold')
ax.set_xlabel('Timestep')
ax.set_ylabel('Valeur')
ax.legend()
ax.grid(True, alpha=0.3)

# Distribution
ax = axes[1]
ax.hist(generated_samples.flatten(), bins=50, edgecolor='black', alpha=0.7)
ax.set_title('Distribution des Valeurs G√©n√©r√©es', fontweight='bold')
ax.set_xlabel('Valeur')
ax.set_ylabel('Fr√©quence')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(output_dir / 'generated_samples.png', dpi=150)
print(f"\n‚úì G√©n√©rations sauvegard√©es: {output_dir / 'generated_samples.png'}")
plt.close()

# ============================================================================
# TEST 6: COMPARAISON AVEC DONN√âES R√âELLES
# ============================================================================

print(f"\n{'='*70}")
print("üìä TEST 6: Comparaison avec donn√©es r√©elles")
print("="*70)

if train_data is not None:
    # Statistiques des donn√©es r√©elles (normalis√©es)
    real_mean = train_data.mean()
    real_std = train_data.std()
    
    print(f"\nüìä Donn√©es r√©elles (normalis√©es):")
    print(f"   Mean: {real_mean:.4f}")
    print(f"   Std:  {real_std:.4f}")
    
    print(f"\nüìä Donn√©es g√©n√©r√©es:")
    print(f"   Mean: {gen_mean:.4f}")
    print(f"   Std:  {gen_std:.4f}")
    
    # Test de Kolmogorov-Smirnov
    ks_stat, ks_pvalue = stats.ks_2samp(
        train_data.flatten()[:10000],  # √âchantillon
        generated_samples.flatten()
    )
    
    print(f"\nüìà Test de Kolmogorov-Smirnov:")
    print(f"   Statistique: {ks_stat:.4f}")
    print(f"   P-value: {ks_pvalue:.4f}")
    
    if ks_pvalue > 0.05:
        print(f"   ‚úÖ Les distributions sont statistiquement similaires")
    else:
        print(f"   ‚ö†Ô∏è  Les distributions diff√®rent significativement")

# ============================================================================
# RAPPORT FINAL
# ============================================================================

print(f"\n{'='*70}")
print("üìã RAPPORT FINAL")
print("="*70)

report = {
    'collapse_severity': collapse_severity,
    'kl_divergence': float(final_kl) if 'final_kl' in locals() else None,
    'reconstruction_mse': float(mse) if 'mse' in locals() else None,
    'diversity': diversity if 'diversity' in locals() else None,
    'inactive_dimensions': int(inactive_dims) if 'inactive_dims' in locals() else None,
    'total_dimensions': int(model.latent_dim),
    'inter_sample_variance': float(inter_sample_var) if 'inter_sample_var' in locals() else None,
}

print(f"\nüéØ R√©sum√©:")
print(f"   Posterior Collapse: {collapse_severity}")
print(f"   KL Divergence: {report['kl_divergence']:.4f}" if report['kl_divergence'] else "   KL Divergence: N/A")
print(f"   MSE Reconstruction: {report['reconstruction_mse']:.4f}" if report['reconstruction_mse'] else "   MSE: N/A")
print(f"   Diversit√©: {report['diversity']}" if report['diversity'] else "   Diversit√©: N/A")
print(f"   Dimensions actives: {report['total_dimensions'] - report['inactive_dimensions']}/{report['total_dimensions']}" if report['inactive_dimensions'] is not None else "")

# Verdict global
print(f"\nüèÜ VERDICT GLOBAL:")

score = 0
issues = []

if collapse_severity == "NONE":
    score += 3
    print(f"   ‚úÖ Pas de posterior collapse")
elif collapse_severity in ["MEDIUM", "UNKNOWN"]:
    score += 1
    issues.append("Risque de collapse")
    print(f"   ‚ö†Ô∏è  Risque de posterior collapse")
else:
    issues.append("Posterior collapse d√©tect√©")
    print(f"   ‚ùå Posterior collapse d√©tect√©")

if diversity == "HIGH":
    score += 2
    print(f"   ‚úÖ Bonne diversit√© des g√©n√©rations")
elif diversity == "MEDIUM":
    score += 1
    issues.append("Diversit√© limit√©e")
    print(f"   ‚ö†Ô∏è  Diversit√© limit√©e")
else:
    issues.append("Tr√®s peu de diversit√©")
    print(f"   ‚ùå Tr√®s peu de diversit√©")

if report['inactive_dimensions'] is not None:
    active_ratio = 1 - (report['inactive_dimensions'] / report['total_dimensions'])
    if active_ratio > 0.8:
        score += 1
        print(f"   ‚úÖ Espace latent bien utilis√©")
    elif active_ratio > 0.5:
        issues.append("Sous-utilisation de l'espace latent")
        print(f"   ‚ö†Ô∏è  Sous-utilisation de l'espace latent")
    else:
        issues.append("Espace latent tr√®s peu utilis√©")
        print(f"   ‚ùå Espace latent tr√®s peu utilis√©")

print(f"\n‚≠ê Score: {score}/6")

if score >= 5:
    print(f"   ‚úÖ EXCELLENT - Le mod√®le fonctionne tr√®s bien !")
    recommendation = "Le mod√®le est pr√™t pour la g√©n√©ration de sc√©narios"
elif score >= 3:
    print(f"   ‚ö†Ô∏è  ACCEPTABLE - Quelques am√©liorations possibles")
    recommendation = "Le mod√®le fonctionne mais pourrait √™tre am√©lior√©"
else:
    print(f"   ‚ùå PROBL√âMATIQUE - R√©-entra√Ænement recommand√©")
    recommendation = "Utilisez train_lstm_vae_anticollapse.py"

if issues:
    print(f"\n‚ö†Ô∏è  Probl√®mes d√©tect√©s:")
    for issue in issues:
        print(f"   - {issue}")

print(f"\nüí° Recommandation:")
print(f"   {recommendation}")

# Sauvegarder le rapport
report['score'] = score
report['recommendation'] = recommendation
report['issues'] = issues

with open(output_dir / 'evaluation_report.json', 'w') as f:
    json.dump(report, f, indent=2)

print(f"\n‚úì Rapport sauvegard√©: {output_dir / 'evaluation_report.json'}")

print(f"\n{'='*70}")
print("üìÅ Fichiers cr√©√©s:")
print(f"   {output_dir}/reconstruction_test.png")
print(f"   {output_dir}/latent_space_analysis.png")
print(f"   {output_dir}/generated_samples.png")
print(f"   {output_dir}/evaluation_report.json")
print("="*70)